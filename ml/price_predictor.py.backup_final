# ml/price_predictor.py - COMPLETE FIXED VERSION with Reality Check + Logging Integration
import numpy as np
import pandas as pd
import joblib
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import os
import logging
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')

# Ustawienie podstawowego poziomu logowania
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class OptimizedPricePredictionModel:
    """
    OPTIMIZED: Multi-algorithm ML model with advanced feature engineering
    """
    
    def __init__(self, model_type: str = "ensemble"):
        self.model_type = model_type.lower()
        self.models = {}
        self.scalers = {}
        self.feature_scaler = RobustScaler()  # More robust than StandardScaler
        self.is_trained = False
        
        # Enhanced model configurations
        self.model_configs = {
            'random_forest': {
                'model': RandomForestRegressor,
                'params': {
                    'n_estimators': [50, 100, 150],
                    'max_depth': [6, 8, 10, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            'gradient_boost': {
                'model': GradientBoostingRegressor,
                'params': {
                    'n_estimators': [50, 100, 150],
                    'max_depth': [3, 4, 6],
                    'learning_rate': [0.05, 0.1, 0.15],
                    'subsample': [0.8, 0.9, 1.0]
                }
            },
            'extra_trees': {
                'model': ExtraTreesRegressor,
                'params': {
                    'n_estimators': [50, 100],
                    'max_depth': [6, 8, None],
                    'min_samples_split': [2, 5],
                    'min_samples_leaf': [1, 2]
                }
            },
            'ridge': {
                'model': Ridge,
                'params': {
                    'alpha': [0.1, 1.0, 10.0, 100.0],
                    'solver': ['auto', 'svd', 'cholesky']
                }
            },
            'elastic_net': {
                'model': ElasticNet,
                'params': {
                    'alpha': [0.1, 1.0, 10.0],
                    'l1_ratio': [0.1, 0.5, 0.7, 0.9]
                }
            }
        }
        
        # Performance tracking
        self.model_performance = {}
        self.ensemble_weights = {}
        
        # Feature importance tracking
        self.feature_importance = {}
        
        self.logger = logging.getLogger(__name__)
        os.makedirs("ml/models", exist_ok=True)
        
    def prepare_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """FIXED: Advanced feature engineering with proper error handling"""
        try:
            self.logger.info(f"üîç Advanced feature preparation with {len(df)} rows")
            
            df = df.copy().sort_values('timestamp')
            
            # FIXED: Better price column handling
            if 'price' in df.columns and df['price'].notna().sum() > len(df) * 0.3:
                price_col = 'price'
            elif 'amount_out' in df.columns:
                df['price'] = pd.to_numeric(df['amount_out'], errors='coerce')
                price_col = 'price'
            else:
                self.logger.error("‚ùå No valid price column found")
                return pd.DataFrame()
            
            # Clean price data
            df[price_col] = pd.to_numeric(df[price_col], errors='coerce')
            df = df.dropna(subset=[price_col])
            
            if len(df) < 30:
                self.logger.error(f"‚ùå Insufficient data after cleaning: {len(df)}")
                return pd.DataFrame()
            
            # === PRICE FEATURES ===
            df['price_change'] = df[price_col].pct_change().fillna(0)
            df['price_change_abs'] = abs(df['price_change'])
            
            # Multiple timeframe moving averages
            for period in [3, 5, 10, 15, 20]:
                df[f'sma_{period}'] = df[price_col].rolling(period, min_periods=1).mean()
                df[f'price_vs_sma_{period}'] = (df[price_col] - df[f'sma_{period}']) / (df[f'sma_{period}'] + 1e-8)
            
            # Exponential moving averages
            df['ema_5'] = df[price_col].ewm(span=5).mean()
            df['ema_10'] = df[price_col].ewm(span=10).mean()
            df['ema_20'] = df[price_col].ewm(span=20).mean()
            
            # Price momentum features
            df['momentum_3'] = df[price_col] / (df[price_col].shift(3) + 1e-8) - 1
            df['momentum_5'] = df[price_col] / (df[price_col].shift(5) + 1e-8) - 1
            df['momentum_10'] = df[price_col] / (df[price_col].shift(10) + 1e-8) - 1
            
            # === VOLATILITY FEATURES ===
            for period in [5, 10, 15, 20]:
                vol_col = f'volatility_{period}'
                df[vol_col] = df[price_col].rolling(period, min_periods=1).std()
                df[f'volatility_norm_{period}'] = df[vol_col] / (df[price_col] + 1e-8)
            
            # Bollinger Bands
            df['bb_middle'] = df[price_col].rolling(20, min_periods=1).mean()
            df['bb_std'] = df[price_col].rolling(20, min_periods=1).std()
            df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']
            df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']
            
            # FIXED: Safe bollinger band calculations
            bb_range = df['bb_upper'] - df['bb_lower']
            df['bb_position'] = np.where(bb_range > 1e-8, 
                                       (df[price_col] - df['bb_lower']) / bb_range, 
                                       0.5)
            df['bb_width'] = np.where(df['bb_middle'] > 1e-8,
                                    bb_range / df['bb_middle'],
                                    0.01)
            
            # === TECHNICAL INDICATORS ===
            
            # FIXED: Enhanced RSI with better error handling
            if 'rsi' in df.columns:
                df['rsi_clean'] = pd.to_numeric(df['rsi'], errors='coerce').fillna(50.0)
            else:
                df['rsi_clean'] = self._calculate_rsi_fixed(df[price_col])
            
            # Clip RSI to valid range
            df['rsi_clean'] = df['rsi_clean'].clip(0, 100)
            
            df['rsi_oversold'] = (df['rsi_clean'] < 30).astype(int)
            df['rsi_overbought'] = (df['rsi_clean'] > 70).astype(int)
            df['rsi_neutral'] = ((df['rsi_clean'] >= 40) & (df['rsi_clean'] <= 60)).astype(int)
            
            # MACD
            df['macd_line'], df['macd_signal'] = self._calculate_macd_fixed(df[price_col])
            df['macd_histogram'] = df['macd_line'] - df['macd_signal']
            df['macd_bullish'] = (df['macd_histogram'] > 0).astype(int)
            
            # Stochastic Oscillator
            df['stoch_k'], df['stoch_d'] = self._calculate_stochastic_fixed(df[price_col])
            
            # === VOLUME FEATURES ===
            if 'volume' in df.columns:
                df['volume_clean'] = pd.to_numeric(df['volume'], errors='coerce').fillna(df['amount_in'] if 'amount_in' in df.columns else 1.0)
            elif 'amount_in' in df.columns:
                df['volume_clean'] = pd.to_numeric(df['amount_in'], errors='coerce').fillna(1.0)
            else:
                df['volume_clean'] = 1.0  # Default volume
            
            df['volume_sma_5'] = df['volume_clean'].rolling(5, min_periods=1).mean()
            df['volume_sma_10'] = df['volume_clean'].rolling(10, min_periods=1).mean()
            df['volume_ratio'] = df['volume_clean'] / (df['volume_sma_10'] + 1e-8)
            df['volume_spike'] = (df['volume_ratio'] > 1.5).astype(int)
            
            # === MARKET STRUCTURE FEATURES ===
            
            # Support/Resistance levels
            df['recent_high'] = df[price_col].rolling(10, min_periods=1).max()
            df['recent_low'] = df[price_col].rolling(10, min_periods=1).min()
            
            # FIXED: Safe price position calculation
            price_range = df['recent_high'] - df['recent_low']
            df['price_position'] = np.where(price_range > 1e-8,
                                          (df[price_col] - df['recent_low']) / price_range,
                                          0.5)
            
            # FIXED: Trend strength calculation
            df['trend_strength'] = abs(df['sma_5'] - df['sma_20']) / (df['sma_20'] + 1e-8)
            df['trend_direction'] = np.where(df['sma_5'] > df['sma_20'], 1, -1)
            
            # Price acceleration
            df['price_acceleration'] = df['price_change'].diff().fillna(0)
            
            # === TIME-BASED FEATURES ===
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)
            
            # === MARKET REGIME FEATURES - FIXED ===
            
            # FIXED: Volatility regime using quantiles
            if 'volatility_20' in df.columns:
                vol_q75 = df['volatility_20'].quantile(0.75)
                vol_q25 = df['volatility_20'].quantile(0.25)
                
                df['vol_regime'] = np.where(df['volatility_20'] > vol_q75, 2,      # High vol
                                   np.where(df['volatility_20'] > vol_q25, 1, 0))   # Medium/Low vol
            else:
                # Fallback if volatility_20 doesn't exist
                df['vol_regime'] = 1  # Default to medium volatility
            
            # FIXED: Trend regime using actual trend strength
            trend_q70 = df['trend_strength'].quantile(0.7)
            trend_q30 = df['trend_strength'].quantile(0.3)
            
            df['trend_regime'] = np.where(df['trend_strength'] > trend_q70, 2,      # Strong trend
                                        np.where(df['trend_strength'] > trend_q30, 1, 0))   # Weak/No trend
            
            # === TARGET VARIABLE ===
            df['target'] = df[price_col].shift(-1)
            
            # === FEATURE SELECTION ===
            feature_columns = [
                # Price features
                'price_change', 'price_change_abs', 'price_vs_sma_5', 'price_vs_sma_10', 'price_vs_sma_20',
                'momentum_3', 'momentum_5', 'momentum_10',
                
                # Volatility features
                'volatility_5', 'volatility_10', 'volatility_norm_5', 'volatility_norm_10',
                'bb_position', 'bb_width',
                
                # Technical indicators
                'rsi_clean', 'rsi_oversold', 'rsi_overbought', 'rsi_neutral',
                'macd_histogram', 'macd_bullish', 'stoch_k', 'stoch_d',
                
                # Volume features
                'volume_ratio', 'volume_spike',
                
                # Market structure
                'price_position', 'trend_strength', 'trend_direction', 'price_acceleration',
                
                # Time features
                'hour', 'day_of_week', 'is_weekend', 'is_night',
                
                # Regime features
                'vol_regime', 'trend_regime'
            ]
            
            # FIXED: Keep only existing columns with valid data
            available_features = []
            for col in feature_columns:
                if col in df.columns and not df[col].isna().all() and not np.isinf(df[col]).all():
                    available_features.append(col)
            
            self.logger.info(f"üìã Available features after filtering: {len(available_features)}")
            
            if len(available_features) < 8:
                self.logger.error(f"‚ùå Too few valid features: {len(available_features)}")
                return pd.DataFrame()
            
            # Clean data
            df = df[:-1]  # Remove last row (no target)
            
            # FIXED: Better NaN handling
            for col in available_features:
                if df[col].isna().any():
                    # Use median for numeric features, mode for categorical
                    if col in ['vol_regime', 'trend_regime', 'rsi_oversold', 'rsi_overbought', 'rsi_neutral', 'macd_bullish', 'volume_spike', 'is_weekend', 'is_night']:
                        df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 0)
                    else:
                        df[col] = df[col].fillna(df[col].median())
                
                # Handle infinite values
                if np.isinf(df[col]).any():
                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                    df[col] = df[col].fillna(df[col].median())
            
            # Fill target variable
            df['target'] = df['target'].fillna(method='ffill').fillna(method='bfill')
            final_clean = df.dropna(subset=['target'])
            
            if len(final_clean) < 20:
                self.logger.error(f"‚ùå Too few samples after cleaning: {len(final_clean)}")
                return pd.DataFrame()
            
            result_df = final_clean[available_features + ['target']].copy()
            
            self.logger.info(f"‚úÖ Advanced features prepared: {result_df.shape}")
            self.logger.info(f"   Features: {len(available_features)}")
            self.logger.info(f"   Samples: {len(result_df)}")
            
            return result_df
            
        except Exception as e:
            self.logger.error(f"‚ùå Advanced feature preparation failed: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def _calculate_rsi_fixed(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """FIXED: Enhanced RSI calculation with better error handling"""
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()
            
            # FIXED: Better division by zero handling
            rs = gain / (loss + 1e-6)  # Increased epsilon
            rsi = (100 - (100 / (1 + rs))).fillna(50.0)
            
            # Ensure RSI is within valid range
            return rsi.clip(0, 100)
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è RSI calculation error: {e}")
            return pd.Series([50.0] * len(prices), index=prices.index)
    
    def _calculate_macd_fixed(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series]:
        """FIXED: Enhanced MACD calculation"""
        try:
            ema_fast = prices.ewm(span=fast).mean()
            ema_slow = prices.ewm(span=slow).mean() 
            macd_line = ema_fast - ema_slow
            macd_signal = macd_line.ewm(span=signal).mean()
            return macd_line.fillna(0), macd_signal.fillna(0)
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è MACD calculation error: {e}")
            zeros = pd.Series([0.0] * len(prices), index=prices.index)
            return zeros, zeros
    
    def _calculate_stochastic_fixed(self, prices: pd.Series, window: int = 14) -> Tuple[pd.Series, pd.Series]:
        """FIXED: Stochastic Oscillator calculation"""
        try:
            high_roll = prices.rolling(window).max()
            low_roll = prices.rolling(window).min()
            
            # FIXED: Better division by zero handling
            price_range = high_roll - low_roll
            stoch_k = np.where(price_range > 1e-8,
                             100 * (prices - low_roll) / price_range,
                             50.0)
            stoch_k = pd.Series(stoch_k, index=prices.index)
            stoch_d = stoch_k.rolling(3).mean()
            
            return stoch_k.fillna(50), stoch_d.fillna(50)
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Stochastic calculation error: {e}")
            fifties = pd.Series([50.0] * len(prices), index=prices.index)
            return fifties, fifties
    
    def train_ensemble_models(self, df: pd.DataFrame, test_size: float = 0.2) -> Dict:
        """Train multiple models with hyperparameter optimization"""
        try:
            self.logger.info(f"ü§ñ Training ensemble models with {len(df)} samples...")
            
            # Prepare features
            df_features = self.prepare_advanced_features(df)
            
            if df_features.empty or len(df_features) < 50:
                return {'success': False, 'error': f'Insufficient data: {len(df_features)}'}
            
            # Extract features and target
            feature_cols = [col for col in df_features.columns if col != 'target']
            X = df_features[feature_cols].values.astype(np.float64)
            y = df_features['target'].values.astype(np.float64)
            
            # Remove infinite values
            finite_mask = np.isfinite(X).all(axis=1) & np.isfinite(y)
            X, y = X[finite_mask], y[finite_mask]
            
            if len(X) < 30:
                return {'success': False, 'error': f'Too few clean samples: {len(X)}'}
            
            # Train-test split
            split_idx = max(10, int(len(X) * (1 - test_size)))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            # Scale features
            X_train_scaled = self.feature_scaler.fit_transform(X_train)
            X_test_scaled = self.feature_scaler.transform(X_test)
            
            self.logger.info(f"üìä Training on {len(X_train)} samples, testing on {len(X_test)}")
            
            # Train multiple models
            results = {}
            for model_name, config in self.model_configs.items():
                try:
                    self.logger.info(f"üîÑ Training {model_name}...")
                    
                    # Quick training for large datasets
                    if len(X_train) > 200:
                        # Use simplified parameters for speed with n_jobs=-1
                        if model_name == 'random_forest':
                            model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
                        elif model_name == 'gradient_boost':
                            model = GradientBoostingRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
                        elif model_name == 'extra_trees':
                            model = ExtraTreesRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
                        elif model_name == 'ridge':
                            model = Ridge(alpha=1.0)
                        elif model_name == 'elastic_net':
                            model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=2000)
                    else:
                        # Use GridSearchCV for smaller datasets
                        base_model = config['model'](random_state=42)
                        # Simplified parameter grid for speed
                        simplified_params = {}
                        for param, values in config['params'].items():
                            simplified_params[param] = values[:2]  # Take only first 2 values
                        
                        model = GridSearchCV(base_model, simplified_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
                    
                    # Train model
                    model.fit(X_train_scaled, y_train)
                    
                    # Get best estimator if GridSearch
                    if hasattr(model, 'best_estimator_'):
                        best_model = model.best_estimator_
                    else:
                        best_model = model
                    
                    # Make predictions
                    train_pred = best_model.predict(X_train_scaled)
                    test_pred = best_model.predict(X_test_scaled)
                    
                    # Calculate metrics
                    train_mse = mean_squared_error(y_train, train_pred)
                    test_mse = mean_squared_error(y_test, test_pred)
                    test_mae = mean_absolute_error(y_test, test_pred)
                    test_r2 = r2_score(y_test, test_pred)
                    
                    # Direction accuracy
                    if len(y_test) > 1:
                        actual_direction = np.sign(np.diff(y_test))
                        pred_direction = np.sign(np.diff(test_pred))
                        accuracy = np.mean(actual_direction == pred_direction) * 100
                    else:
                        accuracy = 50.0
                    
                    # Store model and performance
                    self.models[model_name] = best_model
                    self.model_performance[model_name] = {
                        'train_mse': train_mse,
                        'test_mse': test_mse,
                        'test_mae': test_mae,
                        'test_r2': test_r2,
                        'accuracy': accuracy,
                        'training_samples': len(X_train)
                    }
                    
                    # Feature importance (if available)
                    if hasattr(best_model, 'feature_importances_'):
                        self.feature_importance[model_name] = dict(zip(feature_cols, best_model.feature_importances_))
                    
                    results[model_name] = {
                        'success': True,
                        'test_r2': test_r2,
                        'accuracy': accuracy,
                        'test_mae': test_mae
                    }
                    
                    self.logger.info(f"‚úÖ {model_name}: R¬≤={test_r2:.3f}, Acc={accuracy:.1f}%, MAE={test_mae:.6f}")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå {model_name} training failed: {e}")
                    results[model_name] = {'success': False, 'error': str(e)}
            
            # Calculate ensemble weights based on performance
            self._calculate_ensemble_weights()
            
            # Mark as trained if at least one model succeeded
            successful_models = [name for name, result in results.items() if result.get('success')]
            if successful_models:
                self.is_trained = True
                self.logger.info(f"‚úÖ Ensemble training complete! Successful models: {successful_models}")
            else:
                self.logger.error("‚ùå All models failed to train")
                return {'success': False, 'error': 'All models failed'}
            
            return {'success': True, 'results': results, 'successful_models': successful_models}
            
        except Exception as e:
            self.logger.error(f"‚ùå Ensemble training error: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def _calculate_ensemble_weights(self):
        """Calculate weights for ensemble based on model performance"""
        weights = {}
        total_score = 0
        
        for model_name, performance in self.model_performance.items():
            # Weight based on R¬≤ score and accuracy
            r2_score = max(0, performance.get('test_r2', 0))
            accuracy = performance.get('accuracy', 0) / 100
            
            # Combined score (R¬≤ weighted more heavily)
            combined_score = 0.7 * r2_score + 0.3 * accuracy
            weights[model_name] = max(0.1, combined_score)  # Minimum weight of 0.1
            total_score += weights[model_name]
        
        # Normalize weights
        if total_score > 0:
            for model_name in weights:
                weights[model_name] /= total_score
        
        self.ensemble_weights = weights
        self.logger.info(f"üìä Ensemble weights: {self.ensemble_weights}")
    
    def predict_ensemble(self, recent_data: pd.DataFrame) -> Dict:
        """Make ensemble prediction using all trained models"""
        try:
            if not self.is_trained or not self.models:
                return {'error': 'No trained models available'}
            
            # Prepare features
            df_features = self.prepare_advanced_features(recent_data)
            if df_features.empty:
                return {'error': 'No valid features generated'}
            
            # Get latest features
            feature_cols = [col for col in df_features.columns if col != 'target']
            latest_features = df_features[feature_cols].iloc[-1:].values
            
            # Scale features
            latest_features_scaled = self.feature_scaler.transform(latest_features)
            
            # Get predictions from all models
            predictions = {}
            for model_name, model in self.models.items():
                try:
                    pred = model.predict(latest_features_scaled)[0]
                    predictions[model_name] = float(pred)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {model_name} prediction failed: {e}")
            
            if not predictions:
                return {'error': 'No valid predictions generated'}
            
            # Calculate weighted ensemble prediction
            if self.ensemble_weights:
                weighted_sum = sum(pred * self.ensemble_weights.get(name, 0) 
                                 for name, pred in predictions.items())
                ensemble_prediction = weighted_sum
                ensemble_confidence = sum(self.ensemble_weights.get(name, 0) 
                                        for name in predictions.keys())
            else:
                # Simple average if no weights
                ensemble_prediction = np.mean(list(predictions.values()))
                ensemble_confidence = 0.5
            
            # Get current price
            if 'price' in recent_data.columns:
                current_price = recent_data['price'].iloc[-1]
            else:
                current_price = recent_data['amount_out'].iloc[-1]
            
            current_price = float(current_price)
            ensemble_prediction = float(ensemble_prediction)
            
            # Calculate metrics
            direction = 'up' if ensemble_prediction > current_price else 'down'
            price_change = ensemble_prediction - current_price
            price_change_pct = (price_change / current_price) * 100
            
            # Enhanced confidence calculation
            model_agreement = self._calculate_model_agreement(predictions, current_price)
            final_confidence = min(0.95, ensemble_confidence * 0.7 + model_agreement * 0.3)
            
            result = {
                'predicted_price': ensemble_prediction,
                'current_price': current_price,
                'price_change': price_change,
                'price_change_pct': price_change_pct,
                'direction': direction,
                'confidence': float(final_confidence),
                'model_count': len(predictions),
                'individual_predictions': predictions,
                'ensemble_weights': self.ensemble_weights,
                'model_agreement': model_agreement,
                'prediction_time': datetime.now().isoformat(),
                'features_used': len(feature_cols)
            }
            
            self.logger.info(f"üéØ Ensemble: {direction.upper()} ${ensemble_prediction:.4f} "
                           f"(confidence: {final_confidence:.2f}, agreement: {model_agreement:.2f})")
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Ensemble prediction error: {e}")
            return {'error': str(e)}
    
    def _calculate_model_agreement(self, predictions: Dict, current_price: float) -> float:
        """Calculate how much models agree on direction"""
        if len(predictions) < 2:
            return 0.5
        
        directions = [1 if pred > current_price else -1 for pred in predictions.values()]
        agreement = abs(sum(directions)) / len(directions)
        return agreement
    
    def save_ensemble_models(self):
        """FIXED: Save all trained models"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            for model_name, model in self.models.items():
                filepath = f"ml/models/ensemble_{model_name}_{timestamp}"
                joblib.dump(model, f"{filepath}.pkl")
            
            # Save metadata
            metadata = {
                'feature_scaler': self.feature_scaler,
                'model_performance': self.model_performance,
                'ensemble_weights': self.ensemble_weights,
                'feature_importance': self.feature_importance,
                'training_time': datetime.now().isoformat()
            }
            
            joblib.dump(metadata, f"ml/models/ensemble_metadata_{timestamp}.pkl")
            self.logger.info(f"‚úÖ Ensemble models saved: {timestamp}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error saving ensemble models: {e}")
            return False

class MLTradingIntegration:
    """OPTIMIZED: Enhanced ML integration with multi-model ensemble and Reality Check"""
    
    def __init__(self):
        self.ensemble_model = OptimizedPricePredictionModel('ensemble')
        self.last_training_time = None
        self.training_in_progress = False
        self.logger = logging.getLogger(__name__)
        
    def train_models(self, df: pd.DataFrame):
        """Train ensemble models"""
        if self.training_in_progress:
            return {'success': False, 'error': 'Training already in progress'}
        
        try:
            self.training_in_progress = True
            self.logger.info(f"ü§ñ Starting ensemble training with {len(df)} samples...")
            
            result = self.ensemble_model.train_ensemble_models(df)
            
            if result.get('success'):
                self.last_training_time = datetime.now()
                self.logger.info("‚úÖ Ensemble training completed successfully!")
                
                # Save models
                self.ensemble_model.save_ensemble_models()
            else:
                self.logger.error(f"‚ùå Ensemble training failed: {result.get('error')}")
            
            return result
            
        finally:
            self.training_in_progress = False

    def get_all_historical_data(self) -> pd.DataFrame:
        """Get ALL historical data from PostgreSQL + CSV for ML training"""
        try:
            from database.db_manager import get_db_manager

            # Try PostgreSQL first
            try:
                db_manager = get_db_manager()
                df = db_manager.get_all_transactions_for_ml()
                
                if len(df) > 100:
                    self.logger.info(f"‚úÖ Loaded {len(df)} transactions from PostgreSQL for ML")
                    return df
                else:
                    self.logger.warning(f"‚ö†Ô∏è PostgreSQL has only {len(df)} transactions")
            except Exception as e:
                self.logger.error(f"‚ùå PostgreSQL error: {e}")
            
            # Fallback to CSV
            try:
                import os
                csv_path = "data/memory.csv"
                if os.path.exists(csv_path):
                    df = pd.read_csv(csv_path)
                    if len(df) > 50:
                        self.logger.info(f"‚úÖ Loaded {len(df)} transactions from CSV for ML")
                        return df
                    else:
                        self.logger.warning(f"‚ö†Ô∏è CSV has only {len(df)} transactions")
            except Exception as e:
                self.logger.error(f"‚ùå CSV error: {e}")
            
            self.logger.error("‚ùå No historical data available for ML training")
            return pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"‚ùå Error loading historical data: {e}")
            return pd.DataFrame()
    
    def train_models_with_all_data(self):
        """ENHANCED: Train models using ALL available historical data"""
        if self.training_in_progress:
            return {'success': False, 'error': 'Training already in progress'}
        
        try:
            self.training_in_progress = True
            
            # Get ALL historical data
            all_data = self.get_all_historical_data()
            
            if len(all_data) < 100:
                return {'success': False, 'error': f'Insufficient historical data: {len(all_data)} (need 100+)'}
            
                self.logger.info(f"ü§ñ Starting enhanced training with {len(all_data)} historical samples...")
            
                # Train with all historical data
                result = self.ensemble_model.train_ensemble_models(all_data)
            
                if result.get('success'):
                    self.last_training_time = datetime.now()
                    self.logger.info("‚úÖ Enhanced training with ALL historical data completed!")
            
                # Save models
                    self.ensemble_model.save_ensemble_models()
                else:
                    self.logger.error(f"‚ùå Enhanced training failed: {result.get('error')}")
            
                return result
            
        finally:
            self.training_in_progress = False

        def get_ensemble_prediction(self, recent_data: pd.DataFrame) -> Dict:
        """Get enhanced ensemble prediction using ALL available data"""
        # Auto-train if no models - use ALL historical data
        if not self.ensemble_model.is_trained:
            self.logger.info("ü§ñ Auto-training with ALL historical data...")
        
        # Try training with ALL data first
        training_result = self.train_models_with_all_data()

            # Dodaj tƒô funkcjƒô do ml/price_predictor.py

        def load_local_csv_data(self, csv_path='data/memory.csv'):
        """Load and process local CSV data for ML"""
        try:
        if not os.path.exists(csv_path):
            self.logger.warning(f"CSV file not found: {csv_path}")
            return None
            
        df = pd.read_csv(csv_path)
        self.logger.info(f"üìÅ Loaded {len(df)} transactions from local CSV")
        
        # Convert timestamp to datetime
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # Add derived features
        df['price_change'] = df['price'].pct_change()
        df['volume_ma'] = df['volume'].rolling(window=5).mean()
        df['rsi_trend'] = df['rsi'].diff()
        df['impact_abs'] = df['price_impact'].abs()
        
            # Add time-based features
            df['hour'] = df['timestamp'].dt.hour
            df['minute'] = df['timestamp'].dt.minute
            df['second'] = df['timestamp'].dt.second
            
            # Add technical indicators
            df['price_ma_5'] = df['price'].rolling(window=5).mean()
            df['price_ma_10'] = df['price'].rolling(window=10).mean()
            df['volume_ratio'] = df['volume'] / df['volume'].rolling(window=10).mean()
            
            self.logger.info(f"‚úÖ Enhanced local data with {df.shape[1]} features")
            return df
            
        except Exception as e:
            self.logger.error(f"Error loading local CSV: {e}")
            return None

        def get_enhanced_local_prediction(self, recent_data=None):
        """Get ML prediction using local CSV data"""
        try:
            # Load local data
            local_df = self.load_local_csv_data()
            if local_df is None or len(local_df) < 20:
                return None
                
            # Prepare features for prediction
            feature_cols = [
                'price', 'volume', 'rsi', 'price_impact',
                'price_change', 'rsi_trend', 'impact_abs',
                'price_ma_5', 'price_ma_10', 'volume_ratio'
            ]
            
            # Get available features (handle NaN)
            available_features = []
            for col in feature_cols:
                if col in local_df.columns:
                    available_features.append(col)
            
            # Prepare data
            X = local_df[available_features].fillna(method='ffill').fillna(0)
            
            if len(X) < 10:
                return None
                
            # Use last values for prediction
            latest_features = X.iloc[-1:].values
            
            # Simple ensemble prediction
            predictions = []
            
            # Trend prediction (simple)
            recent_prices = local_df['price'].tail(5)
            trend = 'up' if recent_prices.iloc[-1] > recent_prices.iloc[0] else 'down'
            
            # RSI-based prediction
            latest_rsi = local_df['rsi'].iloc[-1]
            if latest_rsi < 30:
                rsi_signal = 'up'  # Oversold
            elif latest_rsi > 70:
                rsi_signal = 'down'  # Overbought
            else:
                rsi_signal = 'neutral'
            
            # Price impact analysis
            avg_impact = local_df['price_impact'].tail(10).mean()
            impact_signal = 'down' if avg_impact > 0.01 else 'up'
            
            # Combine signals
            signals = [trend, rsi_signal, impact_signal]
            up_votes = signals.count('up')
            down_votes = signals.count('down')
            
            if up_votes > down_votes:
                direction = 'up'
                confidence = up_votes / len(signals)
            elif down_votes > up_votes:
                direction = 'down'
                confidence = down_votes / len(signals)
        else:
            direction = 'neutral'
            confidence = 0.5
        
        # Predicted price
        price_change_pct = confidence * 0.005 if direction == 'up' else -confidence * 0.005
        current_price = local_df['price'].iloc[-1]
        predicted_price = current_price * (1 + price_change_pct)
        
        prediction = {
            'direction': direction,
            'confidence': confidence,
            'predicted_price': predicted_price,
            'price_change_pct': price_change_pct * 100,
            'current_price': current_price,
            'data_points': len(local_df),
            'signals': {
                'trend': trend,
                'rsi': rsi_signal,
                'impact': impact_signal
            },
            'technical_data': {
                'rsi': latest_rsi,
                'avg_impact': avg_impact,
                'recent_trend': (recent_prices.iloc[-1] - recent_prices.iloc[0]) / recent_prices.iloc[0] * 100
            }
        }
        
        self.logger.info(f"üéØ Local ML Prediction: {direction.upper()} ({confidence:.2f}) -> ${predicted_price:.4f}")
        return prediction
        
    except Exception as e:
        self.logger.error(f"Local prediction error: {e}")
        return None

# Dodaj te≈º tƒô funkcjƒô do get_ensemble_prediction_with_reality_check
def get_ensemble_prediction_with_local_data(self, recent_data=None):
    """Enhanced ensemble prediction using both PostgreSQL and local CSV data"""
    try:
        # Get regular prediction
        regular_prediction = self.get_ensemble_prediction_with_reality_check(recent_data)
        
        # Get local prediction
        local_prediction = self.get_enhanced_local_prediction()
        
        if local_prediction and regular_prediction:
            # Combine predictions
            combined_confidence = (regular_prediction['confidence'] + local_prediction['confidence']) / 2
            
            # Agreement check
            agreement = regular_prediction['direction'] == local_prediction['direction']
            
            if agreement:
                final_direction = regular_prediction['direction']
                final_confidence = min(0.9, combined_confidence * 1.2)  # Boost for agreement
            else:
                # Conflict - use higher confidence
                if regular_prediction['confidence'] > local_prediction['confidence']:
                    final_direction = regular_prediction['direction']
                    final_confidence = regular_prediction['confidence'] * 0.8  # Reduce for conflict
                else:
                    final_direction = local_prediction['direction']
                    final_confidence = local_prediction['confidence'] * 0.8
            
            # Combined prediction
            current_price = local_prediction['current_price']
            price_change = final_confidence * 0.005 if final_direction == 'up' else -final_confidence * 0.005
            
            return {
                'direction': final_direction,
                'confidence': final_confidence,
                'predicted_price': current_price * (1 + price_change),
                'price_change_pct': price_change * 100,
                'model_agreement': 1.0 if agreement else 0.0,
                'model_count': 2,
                'data_sources': ['postgresql', 'local_csv'],
                'local_data_points': local_prediction['data_points'],
                'postgresql_data': len(recent_data) if recent_data is not None else 0,
                'technical_signals': local_prediction['signals']
            }
        
        elif local_prediction:
            # Only local data available
            return local_prediction
        else:
            # Fall back to regular prediction
            return regular_prediction
            
    except Exception as e:
        self.logger.error(f"Enhanced ensemble prediction error: {e}")
        return self.get_ensemble_prediction_with_reality_check(recent_data)
        
        if not training_result.get('success'):
            # Fallback to recent data only
            if len(recent_data) >= 100:
                self.logger.info("ü§ñ Fallback: training with recent data only...")
                training_result = self.train_models(recent_data)
            
            if not training_result.get('success'):
                return {'error': f'Auto-training failed: {training_result.get("error")}'}
    
        if not self.ensemble_model.is_trained:
            return {'error': f'No trained models available'}
    
        return self.ensemble_model.predict_ensemble(recent_data)
    
    def get_ensemble_prediction_with_reality_check(self, recent_data: pd.DataFrame) -> Dict:
        """ENHANCED: Ensemble prediction WITH REALITY CHECK for better performance"""
        
        # Get base prediction
        """Get detailed performance metrics for all models"""
        if not self.ensemble_model.model_performance:
            return {}
        
        performance = {}
        for model_name, metrics in self.ensemble_model.model_performance.items():
            performance[model_name] = {
                'accuracy': metrics.get('accuracy', 0),
                'r2': metrics.get('test_r2', 0),
                'mae': metrics.get('test_mae', 0),
                'training_samples': metrics.get('training_samples', 0),
                'last_trained': self.last_training_time.strftime('%Y-%m-%d %H:%M:%S') if self.last_training_time else 'Never',
                'ensemble_weight': self.ensemble_model.ensemble_weights.get(model_name, 0)
            }
        
        return performance
    
    def should_retrain(self) -> bool:
        """Check if models should be retrained"""
        if self.last_training_time is None:
            return True
        
        # Retrain every 2 hours for optimal performance (reduced from 4h)
        time_since_training = datetime.now() - self.last_training_time
        return time_since_training > timedelta(hours=2)
    
    def get_feature_importance(self) -> Dict:
        """Get feature importance from ensemble models"""
        return self.ensemble_model.feature_importance

    def get_all_historical_data(self) -> pd.DataFrame:
        """Get ALL historical data from PostgreSQL + CSV for ML training"""
        try:
            from database.db_manager import get_db_manager
            
            # Try PostgreSQL first
            try:
                db_manager = get_db_manager()
                df = db_manager.get_all_transactions_for_ml()
                
                if len(df) > 100:
                    self.logger.info(f"‚úÖ Loaded {len(df)} transactions from PostgreSQL for ML")
                    return df
                else:
                    self.logger.warning(f"‚ö†Ô∏è PostgreSQL has only {len(df)} transactions")
            except Exception as e:
                self.logger.error(f"‚ùå PostgreSQL error: {e}")
            
            # Fallback to CSV
            try:
                import os
                csv_path = "data/memory.csv"
                if os.path.exists(csv_path):
                    df = pd.read_csv(csv_path)
                    if len(df) > 50:
                        self.logger.info(f"‚úÖ Loaded {len(df)} transactions from CSV for ML")
                        return df
                    else:
                        self.logger.warning(f"‚ö†Ô∏è CSV has only {len(df)} transactions")
            except Exception as e:
                self.logger.error(f"‚ùå CSV error: {e}")
            
            self.logger.error("‚ùå No historical data available for ML training")
            return pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"‚ùå Error loading historical data: {e}")
            return pd.DataFrame()
    
    def train_models_with_all_data(self):
        """ENHANCED: Train models using ALL available historical data"""
        if self.training_in_progress:
            return {'success': False, 'error': 'Training already in progress'}
        
        try:
            self.training_in_progress = True
            
            # Get ALL historical data
            all_data = self.get_all_historical_data()
            
            if len(all_data) < 100:
                return {'success': False, 'error': f'Insufficient historical data: {len(all_data)} (need 100+)'}
            
            self.logger.info(f"ü§ñ Starting enhanced training with {len(all_data)} historical samples...")
            
            # Train with all historical data
            result = self.ensemble_model.train_ensemble_models(all_data)
            
            if result.get('success'):
                self.last_training_time = datetime.now()
                self.logger.info("‚úÖ Enhanced training with ALL historical data completed!")
                
                # Save models
                self.ensemble_model.save_ensemble_models()
            else:
                self.logger.error(f"‚ùå Enhanced training failed: {result.get('error')}")
            
            return result
            
        finally:
            self.training_in_progress = False