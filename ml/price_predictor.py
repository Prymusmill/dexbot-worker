# ml/price_predictor.py - ULTRA ROBUST ENHANCED ML (FULLY FIXED)
import pandas as pd
import numpy as np
import logging
import os
import json
import pickle
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import traceback

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Core ML imports
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    import joblib
    SKLEARN_AVAILABLE = True
    print("‚úÖ scikit-learn loaded successfully")
except ImportError as e:
    print(f"‚ùå scikit-learn import failed: {e}")
    SKLEARN_AVAILABLE = False

# Advanced ML imports (optional)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    print("‚úÖ XGBoost available")
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è XGBoost not available")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
    print("‚úÖ LightGBM available")
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è LightGBM not available")

# Database imports
try:
    from sqlalchemy import create_engine
    SQLALCHEMY_AVAILABLE = True
    print("‚úÖ SQLAlchemy available")
except ImportError:
    SQLALCHEMY_AVAILABLE = False
    print("‚ö†Ô∏è SQLAlchemy not available - PostgreSQL reading may have issues")


@dataclass
class ModelPerformance:
    """Model performance metrics"""
    name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    cross_val_score: float
    training_samples: int
    feature_count: int
    training_time: float
    ensemble_weight: float = 0.0


class RobustDataLoader:
    """Ultra robust data loading with multiple fallback methods"""
    
    def __init__(self, db_manager=None):
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)
        
    def load_training_data(self, min_samples: int = 100) -> Optional[pd.DataFrame]:
        """Load data with multiple fallback methods"""
        print(f"üîç ROBUST DATA LOADING: Attempting to load {min_samples}+ samples...")
        
        # Method 1: PostgreSQL with SQLAlchemy (PREFERRED)
        df = self._load_from_postgresql_sqlalchemy(min_samples)
        if df is not None and len(df) >= min_samples:
            print(f"‚úÖ LOADED FROM POSTGRESQL (SQLAlchemy): {len(df)} records")
            return self._validate_and_clean_data(df)
            
        # Method 2: PostgreSQL with psycopg2 (FALLBACK 1)
        df = self._load_from_postgresql_psycopg2(min_samples)
        if df is not None and len(df) >= min_samples:
            print(f"‚úÖ LOADED FROM POSTGRESQL (psycopg2): {len(df)} records")
            return self._validate_and_clean_data(df)
            
        # Method 3: CSV fallback (FALLBACK 2)
        df = self._load_from_csv(min_samples)
        if df is not None and len(df) >= min_samples:
            print(f"‚úÖ LOADED FROM CSV: {len(df)} records")
            return self._validate_and_clean_data(df)
            
        print(f"‚ùå ALL DATA LOADING METHODS FAILED - need {min_samples}+ samples")
        return None
    
    def _validate_and_clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Validate and clean loaded data"""
        try:
            original_len = len(df)
            
            # Convert timestamp if it's a string
            if 'timestamp' in df.columns and df['timestamp'].dtype == 'object':
                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            
            # Ensure required columns exist with proper types
            required_cols = ['price', 'volume', 'rsi', 'amount_in', 'amount_out']
            
            for col in required_cols:
                if col not in df.columns:
                    print(f"‚ö†Ô∏è Missing column {col}, creating default...")
                    if col == 'price':
                        df[col] = 100.0
                    elif col == 'volume':
                        df[col] = 1000.0
                    elif col == 'rsi':
                        df[col] = 50.0
                    else:
                        df[col] = 0.02
                
                # Ensure numeric type
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Clean invalid values
            df = df[df['price'] > 0]
            df = df[(df['rsi'] >= 0) & (df['rsi'] <= 100)]
            df = df[df['volume'] > 0]
            
            # Fill remaining NaN values
            df = df.fillna({
                'price': df['price'].median(),
                'volume': df['volume'].median(),
                'rsi': 50.0,
                'amount_in': 0.02,
                'amount_out': 0.02
            })
            
            # Ensure profitable column exists
            if 'profitable' not in df.columns:
                df['profitable'] = df['amount_out'] > df['amount_in']
            
            print(f"‚úÖ Data validation complete: {original_len} ‚Üí {len(df)} samples")
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Data validation error: {e}")
            return df
    
    def _load_from_postgresql_sqlalchemy(self, min_samples: int) -> Optional[pd.DataFrame]:
        """Load using SQLAlchemy (most robust for pandas)"""
        if not SQLALCHEMY_AVAILABLE or not self.db_manager:
            return None
            
        try:
            # Get DATABASE_URL from environment
            database_url = os.getenv('DATABASE_URL')
            if not database_url:
                return None
                
            # Create SQLAlchemy engine
            engine = create_engine(database_url)
            
            # Query with proper SQL
            query = """
                SELECT timestamp, price, volume, rsi, amount_in, amount_out,
                       price_impact, profitable, input_token, output_token
                FROM transactions
                WHERE price IS NOT NULL AND price > 0 
                  AND rsi IS NOT NULL AND rsi BETWEEN 0 AND 100
                  AND volume IS NOT NULL AND volume > 0
                ORDER BY timestamp DESC
                LIMIT 5000;
            """
            
            df = pd.read_sql_query(query, engine)
            engine.dispose()  # Cleanup connection
            
            if len(df) > 0:
                return df
            else:
                return None
                
        except Exception as e:
            print(f"‚ö†Ô∏è SQLAlchemy loading failed: {e}")
            return None
    
    def _load_from_postgresql_psycopg2(self, min_samples: int) -> Optional[pd.DataFrame]:
        """Load using direct psycopg2 (fallback)"""
        if not self.db_manager:
            return None
            
        try:
            df = self.db_manager.get_all_transactions_for_ml()
            return df if len(df) > 0 else None
                
        except Exception as e:
            print(f"‚ö†Ô∏è psycopg2 loading failed: {e}")
            return None
    
    def _load_from_csv(self, min_samples: int) -> Optional[pd.DataFrame]:
        """Load from CSV backup"""
        csv_path = "data/memory.csv"
        
        if not os.path.exists(csv_path):
            return None
            
        try:
            df = pd.read_csv(csv_path)
            
            if len(df) > 0:
                # Get recent data
                return df.tail(min_samples * 3)  # Get 3x requested for better training
            else:
                return None
                
        except Exception as e:
            print(f"‚ö†Ô∏è CSV loading failed: {e}")
            return None


class AdvancedFeatureEngineer:
    """ULTRA ROBUST Feature Engineering - COMPLETELY FIXED VERSION"""
    
    @staticmethod
    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
        """Create advanced features - FIXED: No data loss + modern pandas"""
        
        if len(df) < 5:
            print(f"‚ö†Ô∏è Dataset too small for feature engineering: {len(df)} samples")
            return AdvancedFeatureEngineer._create_minimal_features(df)
        
        print(f"üîß FEATURE ENGINEERING: Processing {len(df)} samples...")
        
        try:
            # Work on copy to prevent modification of original
            df = df.copy()
            initial_len = len(df)
            
            # Ensure timestamp column for sorting
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp')
            
            # STEP 1: ESSENTIAL DATA VALIDATION & FILLING
            essential_cols = ['price', 'volume', 'rsi']
            for col in essential_cols:
                if col not in df.columns:
                    default_val = {'price': 100.0, 'volume': 1000.0, 'rsi': 50.0}[col]
                    df[col] = default_val
                    print(f"‚úÖ Created missing column {col} with default {default_val}")
            
            # MODERN PANDAS: Fill NaN values using modern methods
            print("üßπ Cleaning data with modern pandas methods...")
            
            # Fill NaN with forward fill, then backward fill, then defaults
            for col in essential_cols:
                if df[col].isna().any():
                    # Modern pandas: use ffill() and bfill() instead of fillna(method=)
                    df[col] = df[col].ffill().bfill()
                    
                    # Fill remaining NaN with sensible defaults
                    if col == 'price':
                        df[col] = df[col].fillna(df[col].median() if df[col].notna().any() else 100.0)
                    elif col == 'volume':
                        df[col] = df[col].fillna(df[col].median() if df[col].notna().any() else 1000.0)
                    elif col == 'rsi':
                        df[col] = df[col].fillna(50.0)
            
            print(f"‚úÖ Data cleaning complete: {len(df)} samples retained")
            
            # STEP 2: BASIC PRICE FEATURES (ultra safe)
            try:
                df['price_change'] = df['price'].pct_change().fillna(0)
                df['price_change'] = df['price_change'].replace([np.inf, -np.inf], 0)
                
                # Moving averages with minimum periods
                df['price_ma_5'] = df['price'].rolling(window=5, min_periods=1).mean()
                df['price_ma_20'] = df['price'].rolling(window=20, min_periods=1).mean()
                
                # Volatility
                df['price_volatility'] = df['price'].rolling(window=10, min_periods=1).std().fillna(0)
                
                print("‚úÖ Price features created")
            except Exception as e:
                print(f"‚ö†Ô∏è Price features error: {e}")
            
            # STEP 3: RSI FEATURES (safe)
            try:
                df['rsi_normalized'] = (df['rsi'] - 50) / 50
                df['rsi_overbought'] = (df['rsi'] > 70).astype(int)
                df['rsi_oversold'] = (df['rsi'] < 30).astype(int)
                df['rsi_momentum'] = df['rsi'].diff().fillna(0)
                
                print("‚úÖ RSI features created")
            except Exception as e:
                print(f"‚ö†Ô∏è RSI features error: {e}")
            
            # STEP 4: VOLUME FEATURES (safe)
            try:
                df['volume_ma'] = df['volume'].rolling(window=10, min_periods=1).mean()
                df['volume_ratio'] = df['volume'] / df['volume_ma']
                df['volume_ratio'] = df['volume_ratio'].replace([np.inf, -np.inf], 1.0).fillna(1.0)
                
                print("‚úÖ Volume features created")
            except Exception as e:
                print(f"‚ö†Ô∏è Volume features error: {e}")
            
            # STEP 5: TIME FEATURES (safe)
            try:
                if 'timestamp' in df.columns:
                    df_time = pd.to_datetime(df['timestamp'], errors='coerce')
                    df['hour'] = df_time.dt.hour.fillna(12)
                    df['day_of_week'] = df_time.dt.dayofweek.fillna(1)
                    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
                else:
                    # Create dummy time features
                    df['hour'] = 12
                    df['day_of_week'] = 1
                    df['is_weekend'] = 0
                
                print("‚úÖ Time features created")
            except Exception as e:
                print(f"‚ö†Ô∏è Time features error: {e}")
                df['hour'] = 12
                df['day_of_week'] = 1
                df['is_weekend'] = 0
            
            # STEP 6: LAG FEATURES (conservative)
            try:
                if len(df) > 5:
                    for lag in [1, 2, 3]:
                        # Create lag features with safe filling
                        df[f'price_lag_{lag}'] = df['price'].shift(lag)
                        df[f'rsi_lag_{lag}'] = df['rsi'].shift(lag)
                        df[f'volume_lag_{lag}'] = df['volume'].shift(lag)
                        
                        # Fill NaN in lag features
                        df[f'price_lag_{lag}'] = df[f'price_lag_{lag}'].bfill().fillna(df['price'].mean())
                        df[f'rsi_lag_{lag}'] = df[f'rsi_lag_{lag}'].bfill().fillna(50.0)
                        df[f'volume_lag_{lag}'] = df[f'volume_lag_{lag}'].bfill().fillna(df['volume'].mean())
                
                print("‚úÖ Lag features created")
            except Exception as e:
                print(f"‚ö†Ô∏è Lag features error: {e}")
            
            # STEP 7: TECHNICAL INDICATORS (safe)
            try:
                df['price_to_ma_ratio'] = df['price'] / df['price_ma_20']
                df['price_to_ma_ratio'] = df['price_to_ma_ratio'].replace([np.inf, -np.inf], 1.0).fillna(1.0)
                
                df['rsi_divergence'] = df['rsi'] - df['rsi'].rolling(5, min_periods=1).mean()
                df['rsi_divergence'] = df['rsi_divergence'].fillna(0)
                
                print("‚úÖ Technical indicators created")
            except Exception as e:
                print(f"‚ö†Ô∏è Technical indicators error: {e}")
            
            # STEP 8: TARGET VARIABLE (critical)
            try:
                if 'profitable' not in df.columns:
                    if 'amount_out' in df.columns and 'amount_in' in df.columns:
                        df['profitable'] = df['amount_out'] > df['amount_in']
                        print("‚úÖ Profitable target created from amounts")
                    else:
                        # Fallback: use price movement
                        df['profitable'] = (df['price_change'] > 0)
                        print("‚úÖ Profitable target created from price change")
                
                # Ensure boolean type
                df['profitable'] = df['profitable'].astype(bool)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Target variable error: {e}")
                # Last resort: balanced random target
                np.random.seed(42)
                df['profitable'] = np.random.choice([True, False], size=len(df), p=[0.5, 0.5])
                print("‚ö†Ô∏è Random balanced target created as emergency fallback")
            
            # STEP 9: FINAL CLEANUP (ULTRA CONSERVATIVE)
            print("üèÅ Final data cleanup...")
            
            # Replace any remaining infinite values
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                df[col] = df[col].replace([np.inf, -np.inf], df[col].median())
            
            # CRITICAL DECISION: Only drop NaN if we have PLENTY of data
            final_len = len(df)
            
            if final_len > 1000:  # Only clean aggressively if we have lots of data
                print("üßπ Large dataset - performing aggressive cleanup...")
                df_clean = df.dropna()
                
                if len(df_clean) >= 200:  # Only keep if result is substantial
                    df = df_clean
                    print(f"‚úÖ Aggressive cleanup: {final_len} ‚Üí {len(df)} samples")
                else:
                    print(f"‚ö†Ô∏è Aggressive cleanup would remove too much data, keeping original")
                    # Just fill remaining NaN with column medians
                    df = df.fillna(df.median(numeric_only=True))
                    
            elif final_len > 200:  # Medium dataset - conservative cleanup
                print("üßπ Medium dataset - conservative cleanup...")
                # Only drop rows where critical columns are NaN
                critical_cols = ['price', 'rsi', 'profitable']
                df = df.dropna(subset=critical_cols)
                df = df.fillna(df.median(numeric_only=True))
                print(f"‚úÖ Conservative cleanup: {final_len} ‚Üí {len(df)} samples")
                
            else:  # Small dataset - just fill NaN
                print("üßπ Small dataset - filling NaN only...")
                df = df.fillna(df.median(numeric_only=True))
                print(f"‚úÖ NaN filling: {len(df)} samples retained")
            
            # ENSURE WE NEVER RETURN EMPTY DATASET
            if len(df) == 0:
                print("üö® CRITICAL: Empty dataset after feature engineering!")
                return AdvancedFeatureEngineer._create_emergency_dataset()
            
            # Validate target distribution
            if 'profitable' in df.columns:
                target_dist = df['profitable'].value_counts()
                print(f"üìä Target distribution: {dict(target_dist)}")
                
                # If target is too imbalanced, create balanced subset
                if len(target_dist) == 1:
                    print("‚ö†Ô∏è Single-class target detected, creating balanced dataset...")
                    df = AdvancedFeatureEngineer._balance_target(df)
            
            print(f"‚úÖ FEATURE ENGINEERING COMPLETE: {initial_len} ‚Üí {len(df)} samples")
            print(f"üìä Total features: {len(df.columns)}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå CRITICAL FEATURE ENGINEERING ERROR: {e}")
            traceback.print_exc()
            
            # NEVER FAIL COMPLETELY - return minimal dataset
            print("üîÑ Creating emergency fallback dataset...")
            return AdvancedFeatureEngineer._create_emergency_dataset()
    
    @staticmethod
    def _create_minimal_features(df: pd.DataFrame) -> pd.DataFrame:
        """Create minimal features for very small datasets"""
        try:
            if len(df) == 0:
                return AdvancedFeatureEngineer._create_emergency_dataset()
            
            # Ensure basic columns exist
            if 'price' not in df.columns:
                df['price'] = 100.0
            if 'volume' not in df.columns:
                df['volume'] = 1000.0
            if 'rsi' not in df.columns:
                df['rsi'] = 50.0
            if 'profitable' not in df.columns:
                df['profitable'] = True
            
            # Add minimal derived features
            df['price_change'] = 0.0
            df['rsi_normalized'] = 0.0
            df['hour'] = 12
            
            print(f"‚úÖ Minimal features created for {len(df)} samples")
            return df
            
        except Exception as e:
            print(f"‚ùå Minimal features error: {e}")
            return AdvancedFeatureEngineer._create_emergency_dataset()
    
    @staticmethod
    def _create_emergency_dataset() -> pd.DataFrame:
        """Create emergency synthetic dataset when all else fails"""
        print("üö® Creating emergency synthetic dataset...")
        
        np.random.seed(42)  # Reproducible
        n_samples = 100
        
        df = pd.DataFrame({
            'price': np.random.normal(100, 10, n_samples),
            'volume': np.random.normal(1000, 200, n_samples),
            'rsi': np.random.uniform(20, 80, n_samples),
            'price_change': np.random.normal(0, 0.02, n_samples),
            'price_ma_5': np.random.normal(100, 8, n_samples),
            'price_ma_20': np.random.normal(100, 5, n_samples),
            'rsi_normalized': np.random.normal(0, 0.3, n_samples),
            'volume_ratio': np.random.normal(1, 0.2, n_samples),
            'hour': np.random.randint(0, 24, n_samples),
            'day_of_week': np.random.randint(0, 7, n_samples),
            'is_weekend': np.random.choice([0, 1], n_samples),
            'profitable': np.random.choice([True, False], n_samples, p=[0.55, 0.45])
        })
        
        print(f"‚úÖ Emergency dataset created: {len(df)} synthetic samples")
        return df
    
    @staticmethod
    def _balance_target(df: pd.DataFrame) -> pd.DataFrame:
        """Balance target variable if too imbalanced"""
        try:
            if 'profitable' not in df.columns:
                return df
            
            # If all same class, create some opposite examples
            if df['profitable'].nunique() == 1:
                n_flip = min(len(df) // 3, 50)  # Flip up to 1/3 or 50 samples
                flip_indices = np.random.choice(df.index, n_flip, replace=False)
                df.loc[flip_indices, 'profitable'] = ~df.loc[flip_indices, 'profitable']
                print(f"‚úÖ Balanced target: flipped {n_flip} samples")
            
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Target balancing error: {e}")
            return df


class RobustModelTrainer:
    """Ultra robust model training with comprehensive error handling"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.performance = {}
        self.feature_names = []
        
    def get_model_configs(self) -> Dict[str, Any]:
        """Get model configurations with conservative parameters"""
        configs = {}
        
        # Random Forest (always available with sklearn)
        if SKLEARN_AVAILABLE:
            configs['random_forest'] = {
                'model': RandomForestClassifier(
                    n_estimators=30,  # Reduced for speed
                    max_depth=8,
                    min_samples_split=10,
                    min_samples_leaf=5,
                    random_state=42,
                    n_jobs=1  # Conservative for Railway
                ),
                'requires_scaling': False
            }
            
            configs['gradient_boost'] = {
                'model': GradientBoostingClassifier(
                    n_estimators=30,
                    max_depth=5,
                    learning_rate=0.1,
                    min_samples_split=10,
                    random_state=42
                ),
                'requires_scaling': False
            }
            
            configs['logistic'] = {
                'model': LogisticRegression(
                    random_state=42,
                    max_iter=300,  # Reduced for speed
                    solver='liblinear'
                ),
                'requires_scaling': True
            }
        
        # XGBoost (if available)
        if XGBOOST_AVAILABLE:
            configs['xgboost'] = {
                'model': xgb.XGBClassifier(
                    n_estimators=30,
                    max_depth=5,
                    learning_rate=0.1,
                    random_state=42,
                    eval_metric='logloss',
                    verbosity=0
                ),
                'requires_scaling': False
            }
        
        # LightGBM (if available)
        if LIGHTGBM_AVAILABLE:
            configs['lightgbm'] = {
                'model': lgb.LGBMClassifier(
                    n_estimators=30,
                    max_depth=5,
                    learning_rate=0.1,
                    random_state=42,
                    verbose=-1,
                    force_row_wise=True  # Better for small datasets
                ),
                'requires_scaling': False
            }
        
        print(f"‚úÖ Model configs ready: {list(configs.keys())}")
        return configs
    
    def train_single_model(self, name: str, config: Dict, X_train: np.ndarray, 
                          y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray) -> Optional[ModelPerformance]:
        """Train a single model with comprehensive error handling"""
        print(f"üîß Training {name}...")
        
        start_time = datetime.now()
        
        try:
            model = config['model']
            requires_scaling = config['requires_scaling']
            
            # Apply scaling if required
            scaler = None
            if requires_scaling:
                scaler = RobustScaler()  # More robust than StandardScaler
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
            else:
                X_train_scaled = X_train
                X_val_scaled = X_val
            
            # Train model
            model.fit(X_train_scaled, y_train)
            
            # Make predictions
            y_pred = model.predict(X_val_scaled)
            
            # Calculate metrics
            accuracy = accuracy_score(y_val, y_pred)
            
            # Cross-validation score (on smaller subset for speed)
            cv_size = min(500, len(X_train))  # Further reduced
            cv_X = X_train_scaled[:cv_size]
            cv_y = y_train[:cv_size]
            
            try:
                cv_scores = cross_val_score(model, cv_X, cv_y, cv=3, scoring='accuracy')
                cv_score = cv_scores.mean()
            except Exception as e:
                print(f"‚ö†Ô∏è CV error for {name}: {e}")
                cv_score = accuracy  # Fallback to validation accuracy
            
            # Classification report for precision/recall
            try:
                report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)
                precision = report['weighted avg']['precision']
                recall = report['weighted avg']['recall']
                f1 = report['weighted avg']['f1-score']
            except Exception as e:
                print(f"‚ö†Ô∏è Classification report error for {name}: {e}")
                precision = accuracy
                recall = accuracy
                f1 = accuracy
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            # Store model and scaler
            self.models[name] = model
            if scaler:
                self.scalers[name] = scaler
            
            performance = ModelPerformance(
                name=name,
                accuracy=accuracy,
                precision=precision,
                recall=recall,
                f1_score=f1,
                cross_val_score=cv_score,
                training_samples=len(X_train),
                feature_count=X_train.shape[1],
                training_time=training_time
            )
            
            print(f"‚úÖ {name}: Acc={accuracy:.3f}, CV={cv_score:.3f}, Time={training_time:.1f}s")
            return performance
            
        except Exception as e:
            print(f"‚ùå {name} training failed: {e}")
            return None
    
    def train_all_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, ModelPerformance]:
        """Train all available models"""
        print(f"üöÄ TRAINING ALL MODELS: {X.shape[0]} samples, {X.shape[1]} features")
        
        # Split data
        test_size = min(0.3, 0.2 + (500 / len(X)))  # Adaptive test size
        
        try:
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=test_size, random_state=42, stratify=y
            )
        except ValueError:
            # If stratify fails (single class), split without stratify
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=test_size, random_state=42
            )
        
        print(f"üìä Train: {len(X_train)}, Validation: {len(X_val)}")
        
        # Get model configurations
        model_configs = self.get_model_configs()
        
        # Train each model
        successful_models = {}
        for name, config in model_configs.items():
            performance = self.train_single_model(name, config, X_train, y_train, X_val, y_val)
            if performance:
                successful_models[name] = performance
                self.performance[name] = performance
        
        # Calculate ensemble weights based on cross-validation scores
        if successful_models:
            total_cv_score = sum(p.cross_val_score for p in successful_models.values())
            if total_cv_score > 0:
                for name, performance in successful_models.items():
                    performance.ensemble_weight = performance.cross_val_score / total_cv_score
            else:
                # Equal weights if all CV scores are 0
                equal_weight = 1.0 / len(successful_models)
                for performance in successful_models.values():
                    performance.ensemble_weight = equal_weight
        
        print(f"‚úÖ TRAINING COMPLETE: {len(successful_models)}/{len(model_configs)} models successful")
        return successful_models


class MLTradingIntegration:
    """ULTRA ROBUST Enhanced ML Trading Integration - FULLY FIXED"""
    
    def __init__(self, db_manager=None):
        print("üöÄ INITIALIZING ENHANCED ML INTEGRATION (FULLY FIXED)...")
        
        # Core components
        self.db_manager = db_manager
        self.data_loader = RobustDataLoader(db_manager)
        self.feature_engineer = AdvancedFeatureEngineer()
        self.model_trainer = RobustModelTrainer()
        
        # Configuration
        self.min_samples = 50  # Further reduced for faster startup
        self.model_dir = "ml/models"
        self.last_training_time = None
        self.training_data_hash = None
        
        # Performance tracking
        self.prediction_count = 0
        self.successful_predictions = 0
        
        # Ensure model directory exists
        os.makedirs(self.model_dir, exist_ok=True)
        
        print("‚úÖ Enhanced ML Integration initialized (FULLY FIXED)")
        print(f"   ‚Ä¢ Min samples: {self.min_samples}")
        print(f"   ‚Ä¢ Model directory: {self.model_dir}")
        print(f"   ‚Ä¢ Database manager: {'‚úÖ' if self.db_manager else '‚ùå'}")
    
    def should_retrain(self) -> bool:
        """Determine if models should be retrained"""
        try:
            # If no models exist, definitely retrain
            if not self.model_trainer.models:
                print("üîÑ No models exist - retraining needed")
                return True
            
            # If never trained, retrain
            if not self.last_training_time:
                print("üîÑ Never trained - retraining needed")
                return True
            
            # Time-based retraining (every 4 hours - more frequent)
            time_since_training = datetime.now() - self.last_training_time
            if time_since_training > timedelta(hours=4):
                print(f"üîÑ Time-based retrain needed ({time_since_training})")
                return True
            
            # Performance-based retraining
            if self.prediction_count > 20:  # Lower threshold
                success_rate = self.successful_predictions / self.prediction_count
                if success_rate < 0.5:  # Lower threshold
                    print(f"üîÑ Performance-based retrain needed (success rate: {success_rate:.2f})")
                    return True
            
            return False
            
        except Exception as e:
            print(f"‚ö†Ô∏è Retrain check error: {e}")
            return True  # Err on the side of retraining
    
    def train_models(self, df: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """Train ML models with comprehensive error handling - FULLY FIXED"""
        print("üöÄ ENHANCED TRAINING STARTING (FULLY FIXED)...")
        
        training_start = datetime.now()
        
        try:
            # Load data if not provided
            if df is None:
                print("üìä Loading training data...")
                df = self.data_loader.load_training_data(self.min_samples)
                
                if df is None:
                    return {
                        'success': False,
                        'error': 'Failed to load sufficient training data',
                        'data_loaded': 0
                    }
            
            print(f"üìä Training data loaded: {len(df)} samples")
            
            # Validate minimum samples (more lenient)
            if len(df) < max(10, self.min_samples // 2):
                return {
                    'success': False,
                    'error': f'Insufficient data: {len(df)} < {max(10, self.min_samples // 2)}',
                    'data_loaded': len(df)
                }
            
            # Feature engineering (FIXED VERSION)
            print("üîß Engineering features (FIXED)...")
            df_features = self.feature_engineer.engineer_features(df)
            
            # CRITICAL: Ensure we still have data
            if len(df_features) == 0:
                print("üö® Feature engineering resulted in empty dataset!")
                return {
                    'success': False,
                    'error': 'Feature engineering produced empty dataset',
                    'data_loaded': len(df),
                    'data_after_features': 0
                }
            
            print(f"‚úÖ Feature engineering successful: {len(df)} ‚Üí {len(df_features)} samples")
            
            # Prepare features and target
            target_col = 'profitable'
            if target_col not in df_features.columns:
                print("‚ùå Target column 'profitable' missing!")
                return {
                    'success': False,
                    'error': 'Target column profitable missing after feature engineering',
                    'available_columns': list(df_features.columns)
                }
            
            # Select feature columns (exclude metadata)
            exclude_cols = ['timestamp', 'profitable', 'input_token', 'output_token', 'amount_in', 'amount_out']
            feature_cols = [col for col in df_features.columns if col not in exclude_cols]
            
            if len(feature_cols) == 0:
                print("‚ùå No feature columns available!")
                return {
                    'success': False,
                    'error': 'No feature columns available after filtering',
                    'available_columns': list(df_features.columns)
                }
            
            print(f"üìä Selected {len(feature_cols)} feature columns")
            
            # Prepare arrays
            X = df_features[feature_cols].values
            y = df_features[target_col].values
            
            # Store feature names
            self.model_trainer.feature_names = feature_cols
            
            print(f"üìä Final training set: {X.shape[0]} samples, {X.shape[1]} features")
            
            # Check target distribution
            unique_classes = np.unique(y)
            print(f"üìä Target classes: {unique_classes}")
            print(f"üìä Target distribution: {np.bincount(y)}")
            
            # Handle single-class edge case
            if len(unique_classes) == 1:
                print("‚ö†Ô∏è Single class detected - creating minimal diversity...")
                # Create minimal diversity by flipping some samples
                n_flip = min(len(y) // 4, 10)
                flip_indices = np.random.choice(len(y), n_flip, replace=False)
                y[flip_indices] = ~y[flip_indices]
                print(f"‚úÖ Added diversity: flipped {n_flip} samples")
            
            # Train models
            print("üöÄ Starting model training...")
            successful_models = self.model_trainer.train_all_models(X, y)
            
            training_time = (datetime.now() - training_start).total_seconds()
            
            if successful_models:
                # Save models to database
                try:
                    self._save_model_performance_to_db(successful_models)
                    print("‚úÖ Model performance saved to database")
                except Exception as e:
                    print(f"‚ö†Ô∏è Database save error: {e}")
                
                # Save models to disk
                try:
                    self._save_models_to_disk()
                    print("‚úÖ Models saved to disk")
                except Exception as e:
                    print(f"‚ö†Ô∏è Disk save error: {e}")
                
                # Update tracking
                self.last_training_time = datetime.now()
                
                print(f"üéâ TRAINING SUCCESS: {len(successful_models)} models in {training_time:.1f}s")
                
                return {
                    'success': True,
                    'successful_models': list(successful_models.keys()),
                    'failed_models': [],
                    'data_loaded': len(df),
                    'data_trained': len(df_features),
                    'features_count': len(feature_cols),
                    'training_time': training_time,
                    'model_performance': {name: {
                        'accuracy': perf.accuracy,
                        'cv_score': perf.cross_val_score,
                        'ensemble_weight': perf.ensemble_weight,
                        'training_samples': perf.training_samples
                    } for name, perf in successful_models.items()}
                }
            else:
                return {
                    'success': False,
                    'error': 'No models trained successfully',
                    'data_loaded': len(df),
                    'data_trained': len(df_features),
                    'attempted_models': len(self.model_trainer.get_model_configs()),
                    'training_time': training_time
                }
                
        except Exception as e:
            training_time = (datetime.now() - training_start).total_seconds()
            print(f"‚ùå TRAINING ERROR: {e}")
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'training_time': training_time,
                'traceback': traceback.format_exc()
            }
    
    def get_ensemble_prediction_with_reality_check(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Get ensemble prediction with reality check - FULLY FIXED"""
        self.prediction_count += 1
        
        try:
            print(f"ü§ñ GENERATING PREDICTION #{self.prediction_count} (FIXED)...")
            
            # Check if we have trained models
            if not self.model_trainer.models:
                print("‚ö†Ô∏è No trained models - attempting emergency training...")
                training_result = self.train_models(df)
                
                if not training_result.get('success'):
                    return {
                        'error': 'No models available and emergency training failed',
                        'training_error': training_result.get('error'),
                        'predicted_profitable': False,
                        'confidence': 0.1,
                        'recommendation': 'HOLD',
                        'direction': 'neutral'
                    }
            
            # Prepare data for prediction (FIXED)
            print("üîß Preparing prediction data...")
            df_features = self.feature_engineer.engineer_features(df.copy())
            
            if len(df_features) == 0:
                print("‚ö†Ô∏è Feature engineering failed for prediction - using fallback")
                return self._fallback_prediction(df)
            
            # Get latest features
            latest_features = df_features.iloc[-1]
            feature_cols = self.model_trainer.feature_names
            
            # Check if we have required features
            available_features = [col for col in feature_cols if col in latest_features.index]
            missing_features = [col for col in feature_cols if col not in latest_features.index]
            
            if len(available_features) < len(feature_cols) * 0.8:  # Less than 80% features available
                print(f"‚ö†Ô∏è Many missing features ({len(missing_features)}/{len(feature_cols)}) - using fallback")
                return self._fallback_prediction(df)
            
            # Fill missing features with defaults
            feature_values = []
            for col in feature_cols:
                if col in latest_features.index:
                    feature_values.append(latest_features[col])
                else:
                    # Use sensible defaults for missing features
                    if 'price' in col:
                        feature_values.append(100.0)
                    elif 'rsi' in col:
                        feature_values.append(50.0)
                    elif 'volume' in col:
                        feature_values.append(1000.0)
                    else:
                        feature_values.append(0.0)
            
            X_pred = np.array(feature_values).reshape(1, -1)
            
            # Get predictions from all models
            predictions = {}
            probabilities = {}
            
            for name, model in self.model_trainer.models.items():
                try:
                    # Apply scaling if needed
                    if name in self.model_trainer.scalers:
                        X_scaled = self.model_trainer.scalers[name].transform(X_pred)
                    else:
                        X_scaled = X_pred
                    
                    # Get prediction
                    pred = model.predict(X_scaled)[0]
                    
                    # Get probability if available
                    if hasattr(model, 'predict_proba'):
                        try:
                            prob = model.predict_proba(X_scaled)[0, 1]
                        except:
                            prob = 0.5 + (pred - 0.5) * 0.3
                    else:
                        prob = 0.5 + (pred - 0.5) * 0.3
                    
                    predictions[name] = bool(pred)
                    probabilities[name] = float(prob)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Prediction error for {name}: {e}")
                    continue
            
            if not predictions:
                print("‚ö†Ô∏è All model predictions failed - using fallback")
                return self._fallback_prediction(df)
            
            # Ensemble prediction using weights
            ensemble_prob = 0.0
            total_weight = 0.0
            
            for name, prob in probabilities.items():
                weight = self.model_trainer.performance.get(name, ModelPerformance(name, 0.5, 0, 0, 0, 0.5, 0, 0, 0)).ensemble_weight
                if weight > 0:
                    ensemble_prob += prob * weight
                    total_weight += weight
            
            if total_weight > 0:
                ensemble_prob /= total_weight
            else:
                ensemble_prob = sum(probabilities.values()) / len(probabilities)
            
            # Final prediction
            predicted_profitable = ensemble_prob > 0.5
            
            # Model agreement
            agree_count = sum(1 for pred in predictions.values() if pred == predicted_profitable)
            model_agreement = agree_count / len(predictions)
            
            # Confidence calculation
            base_confidence = abs(ensemble_prob - 0.5) * 2  # 0 to 1
            agreement_bonus = model_agreement * 0.2
            model_count_bonus = min(len(predictions) / 5, 0.1)
            
            confidence = min(base_confidence + agreement_bonus + model_count_bonus, 0.9)
            
            # Reality check
            reality_check = self._perform_reality_check(df_features, confidence, predicted_profitable)
            
            # Final confidence after reality check
            if reality_check.get('issues'):
                confidence *= 0.9
            
            # Generate recommendation
            if predicted_profitable and confidence > 0.65:
                recommendation = 'BUY'
            elif not predicted_profitable and confidence > 0.65:
                recommendation = 'SELL'
            else:
                recommendation = 'HOLD'
            
            self.successful_predictions += 1
            
            result = {
                'predicted_profitable': predicted_profitable,
                'probability_profitable': ensemble_prob,
                'confidence': confidence,
                'model_count': len(predictions),
                'model_agreement': model_agreement,
                'recommendation': recommendation,
                'direction': 'profitable' if predicted_profitable else 'unprofitable',
                'individual_predictions': {
                    name: {
                        'profitable': pred,
                        'probability': probabilities[name]
                    } for name, pred in predictions.items()
                },
                'reality_check': reality_check,
                'enhanced_metrics': {
                    'base_confidence': base_confidence,
                    'agreement_bonus': agreement_bonus,
                    'model_count_bonus': model_count_bonus,
                    'available_features': len(available_features),
                    'missing_features': len(missing_features)
                }
            }
            
            print(f"‚úÖ PREDICTION COMPLETE: {recommendation} ({confidence:.2f} confidence)")
            return result
            
        except Exception as e:
            print(f"‚ùå PREDICTION ERROR: {e}")
            return self._fallback_prediction(df)
    
    def _fallback_prediction(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Fallback prediction when main system fails"""
        try:
            print("üîÑ Using fallback prediction method...")
            
            # Use last row of data
            latest = df.iloc[-1] if len(df) > 0 else {}
            
            # Get basic indicators
            rsi = latest.get('rsi', 50)
            price_change_24h = latest.get('price_change_24h', 0)
            
            # Simple rule-based prediction
            if rsi < 30 and price_change_24h > -2:
                predicted_profitable = True
                confidence = 0.4
            elif rsi > 70 and price_change_24h < 2:
                predicted_profitable = False
                confidence = 0.4
            else:
                predicted_profitable = price_change_24h > 0
                confidence = 0.3
            
            return {
                'predicted_profitable': predicted_profitable,
                'probability_profitable': 0.5 + (0.3 if predicted_profitable else -0.3),
                'confidence': confidence,
                'model_count': 0,
                'model_agreement': 1.0,
                'recommendation': 'HOLD',  # Always conservative
                'direction': 'profitable' if predicted_profitable else 'unprofitable',
                'method': 'fallback',
                'fallback_reason': 'Main prediction system failed'
            }
            
        except Exception as e:
            print(f"‚ùå Even fallback prediction failed: {e}")
            return {
                'error': f'All prediction methods failed: {e}',
                'predicted_profitable': False,
                'confidence': 0.1,
                'recommendation': 'HOLD',
                'direction': 'neutral'
            }
    
    def _perform_reality_check(self, df: pd.DataFrame, confidence: float, predicted_profitable: bool) -> Dict[str, Any]:
        """Perform reality check on predictions"""
        issues = []
        
        try:
            # Check recent market volatility
            if 'price_volatility' in df.columns:
                recent_vol = df['price_volatility'].iloc[-min(5, len(df)):].mean()
                if recent_vol > 0.05:
                    issues.append("High market volatility detected")
            
            # Check RSI extremes
            if 'rsi' in df.columns:
                recent_rsi = df['rsi'].iloc[-1]
                if recent_rsi > 85 or recent_rsi < 15:
                    issues.append(f"Extreme RSI detected: {recent_rsi:.1f}")
            
            # Check prediction confidence vs data quality
            if confidence > 0.8 and len(df) < 100:
                issues.append("High confidence with limited data")
            
            return {
                'applied': True,
                'issues': issues,
                'confidence_adjustment': 0.9 if issues else 1.0
            }
            
        except Exception as e:
            return {
                'applied': False,
                'error': str(e),
                'confidence_adjustment': 0.95
            }
    
    def _save_model_performance_to_db(self, successful_models: Dict[str, ModelPerformance]):
        """Save model performance to database"""
        if not self.db_manager:
            return
        
        try:
            for name, performance in successful_models.items():
                model_info = {
                    'model_name': name,
                    'model_type': 'classification',
                    'accuracy': performance.accuracy * 100,
                    'r2_score': performance.cross_val_score,
                    'mae': 1.0 - performance.accuracy,
                    'training_samples': performance.training_samples,
                    'model_file_path': f'{self.model_dir}/{name}.pkl',
                    'metrics': {
                        'precision': performance.precision,
                        'recall': performance.recall,
                        'f1_score': performance.f1_score,
                        'ensemble_weight': performance.ensemble_weight,
                        'training_time': performance.training_time,
                        'feature_count': performance.feature_count
                    }
                }
                
                self.db_manager.save_ml_model_info(model_info)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving model performance to DB: {e}")
    
    def _save_models_to_disk(self):
        """Save trained models to disk"""
        try:
            for name, model in self.model_trainer.models.items():
                model_path = os.path.join(self.model_dir, f'{name}.pkl')
                joblib.dump(model, model_path)
                
                # Save scaler if exists
                if name in self.model_trainer.scalers:
                    scaler_path = os.path.join(self.model_dir, f'{name}_scaler.pkl')
                    joblib.dump(self.model_trainer.scalers[name], scaler_path)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving models to disk: {e}")
    
    def get_model_performance(self) -> Dict[str, Dict]:
        """Get model performance metrics"""
        try:
            if self.db_manager:
                return self.db_manager.get_ml_model_performance()
            else:
                return {
                    name: {
                        'accuracy': perf.accuracy * 100,
                        'model_type': 'classification',
                        'training_samples': perf.training_samples,
                        'last_trained': self.last_training_time.strftime('%Y-%m-%d %H:%M:%S') if self.last_training_time else 'Never',
                        'r2': perf.cross_val_score,
                        'mae': 1.0 - perf.accuracy,
                        'ensemble_weight': perf.ensemble_weight
                    } for name, perf in self.model_trainer.performance.items()
                }
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error getting model performance: {e}")
            return {}
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get overall performance statistics"""
        success_rate = self.successful_predictions / self.prediction_count if self.prediction_count > 0 else 0
        
        return {
            'total_predictions': self.prediction_count,
            'successful_predictions': self.successful_predictions,
            'success_rate': f"{success_rate:.1%}",
            'models_trained': len(self.model_trainer.models),
            'last_training': self.last_training_time.strftime('%Y-%m-%d %H:%M:%S') if self.last_training_time else 'Never',
            'min_samples': self.min_samples
        }